{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo generación de texto usando Redes Neuronales Recurrentes\n",
        "\n",
        "Usando el libro de la divina comedia entrenaremos un modelo de lenguaje para que pueda generar oraciones en español."
      ],
      "metadata": {
        "id": "aTZsqvk2n3Zn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1G6Ydh7JkiEO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento del conjunto de datos"
      ],
      "metadata": {
        "id": "_1vbMoLzq2pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pg57303.txt', 'r', encoding='utf8') as fp:\n",
        "  text=fp.read()\n",
        "start_indx = text.find('_LA DIVINA COMEDIA_')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "text = text[start_indx:end_indx]\n",
        "char_set = set(text)\n",
        "print('Número de caracteres:', len(text))\n",
        "print('Número de caracteres únicos', len(char_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i16rsC6sq2bV",
        "outputId": "bab5f545-cc61-428c-bb0e-70b5e93ac565"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de caracteres: 710326\n",
            "Número de caracteres únicos 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La mayoria de las librerías especializadas en redes neuronales e implementación de redes neuronales recurrentes no trabajan con entradas de formato string, por lo que debemos transformar el texto a formato numérico, para ello crearemos un dictionario en python que asigne a cada carácter a un entero, ```char2int```.\n",
        "\n"
      ],
      "metadata": {
        "id": "U9Kr49uXsObH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32      )\n",
        "\n",
        "print('Forma del texto codificado:', text_encoded.shape)\n",
        "print(text[:20], '== codificación ==>', text_encoded[:20])\n",
        "print(text_encoded[10:18], '== Invertido ==>', ''.join(char_array[text_encoded[10:18]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZVyirflsrxy",
        "outputId": "a8fd84f4-b2ec-483a-f6f9-7cf856acb168"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma del texto codificado: (710326,)\n",
            "_LA DIVINA COMEDIA_\n",
            " == codificación ==> [56 39 28  1 31 36 49 36 41 28  1 30 42 40 32 31 36 28 56  0]\n",
            "[ 1 30 42 40 32 31 36 28] == Invertido ==>  COMEDIA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limitemos el tamaño de la secuencia a 40 caracteres. Además crearemos trozos de 41 caracteres dividiendo el texto original (40 caracteres que corresponden a las características y 1 caracter de etiqueta)"
      ],
      "metadata": {
        "id": "4VbsqLOzuqnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size)]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, text_chunks):\n",
        "    self.text_chunks = text_chunks\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text_chunks)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text_chunk = self.text_chunks[idx]\n",
        "    return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ],
      "metadata": {
        "id": "yRGs_rXEvDhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e481418-059c-4f49-8349-c62889381f98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-377d45116273>:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "  print(' Input (x): ',\n",
        "    repr(''.join(char_array[seq])))\n",
        "  print('Target (y): ',\n",
        "    repr(''.join(char_array[target])))\n",
        "  print()\n",
        "  if i == 1:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEPSnvYGw8yQ",
        "outputId": "5d544dd7-0392-4cd5-ce16-2184242fa8ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input (x):  '_LA DIVINA COMEDIA_\\n\\n\\n\\n\\n                '\n",
            "Target (y):  'LA DIVINA COMEDIA_\\n\\n\\n\\n\\n                 '\n",
            "\n",
            " Input (x):  'LA DIVINA COMEDIA_\\n\\n\\n\\n\\n                 '\n",
            "Target (y):  'A DIVINA COMEDIA_\\n\\n\\n\\n\\n                  '\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente el último paso para preparar este conjunto de datos es transformar el conjunto de datos en lotes"
      ],
      "metadata": {
        "id": "oPZZFDdfxXg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 64\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size,\n",
        "shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "hKzTmCtexj8e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construyendo el modelo"
      ],
      "metadata": {
        "id": "_euUzw-wyNpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.rnn_hidden_size = rnn_hidden_size\n",
        "    self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, hidden, cell):\n",
        "    out = self.embedding(x).unsqueeze(1)\n",
        "    out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "    out = self.fc(out).reshape(out.size(0), -1)\n",
        "    return out, hidden, cell\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "    cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "    return hidden, cell"
      ],
      "metadata": {
        "id": "tTUNn21YyPUd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Especificando los parametros del modelo:"
      ],
      "metadata": {
        "id": "OAnXFGsHzano"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szl_hzqtzXFL",
        "outputId": "6210a282-3749-4f32-ef26-75cd967c2f3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(107, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=107, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función de error y optimizador"
      ],
      "metadata": {
        "id": "2A3k54tpzzbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "dEZTHKj-z2ih"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "X93Kf8_G00LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10000\n",
        "torch.manual_seed(1)\n",
        "for epoch in range(num_epochs):\n",
        "  hidden, cell = model.init_hidden(batch_size)\n",
        "  seq_batch, target_batch = next(iter(seq_dl))\n",
        "  optimizer.zero_grad()\n",
        "  loss = 0\n",
        "  for c in range(seq_length):\n",
        "    pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "    loss += loss_fn(pred, target_batch[:, c])\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  loss = loss.item()/seq_length\n",
        "  if epoch % 500 == 0:\n",
        "    print(f'Epoca {epoch+1} error: {loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmE07aHN01nL",
        "outputId": "2a032f9f-9f47-4bae-c924-8b0e4e49b92d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1 error: 4.6840\n",
            "Epoca 501 error: 1.7526\n",
            "Epoca 1001 error: 1.5795\n",
            "Epoca 1501 error: 1.4643\n",
            "Epoca 2001 error: 1.4153\n",
            "Epoca 2501 error: 1.4199\n",
            "Epoca 3001 error: 1.3493\n",
            "Epoca 3501 error: 1.2824\n",
            "Epoca 4001 error: 1.2088\n",
            "Epoca 4501 error: 1.2145\n",
            "Epoca 5001 error: 1.1935\n",
            "Epoca 5501 error: 1.1973\n",
            "Epoca 6001 error: 1.1875\n",
            "Epoca 6501 error: 1.1574\n",
            "Epoca 7001 error: 1.1902\n",
            "Epoca 7501 error: 1.1000\n",
            "Epoca 8001 error: 1.0689\n",
            "Epoca 8501 error: 1.0893\n",
            "Epoca 9001 error: 1.1177\n",
            "Epoca 9501 error: 1.1151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de texto"
      ],
      "metadata": {
        "id": "N8yU5c_92jhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "torch.manual_seed(1)\n",
        "logits = torch.tensor([[1.0, 1.0, 1.0]])\n",
        "print('Probabilidades:',\n",
        "  nn.functional.softmax(logits, dim=1).numpy()[0]\n",
        "     )\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "print(samples.numpy())"
      ],
      "metadata": {
        "id": "G6ggqAXR2lcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80da62c-6096-4a42-f5ee-63f00c554a35"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades: [0.33333334 0.33333334 0.33333334]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, starting_str,len_generated_text=500,scale_factor=1.0):\n",
        "  encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "  encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "  generated_str = starting_str\n",
        "\n",
        "  model.eval()\n",
        "  hidden, cell = model.init_hidden(1)\n",
        "  for c in range(len(starting_str)-1):\n",
        "    _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "  last_char = encoded_input[:, -1]\n",
        "  for i in range(len_generated_text):\n",
        "    logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "    logits = torch.squeeze(logits, 0)\n",
        "    scaled_logits = logits * scale_factor\n",
        "    m = Categorical(logits=scaled_logits)\n",
        "    last_char = m.sample()\n",
        "    generated_str += str(char_array[last_char])\n",
        "\n",
        "  return generated_str"
      ],
      "metadata": {
        "id": "ZyP-OI2T3Rox"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='eterno'))"
      ],
      "metadata": {
        "id": "cJE4H9jd31ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80c043b-aead-420c-f56f-26660aecb798"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eterno,\n",
            "vuelves ante la misma noche pendiente seas, tus ojos del agua baja? ¿Por qué no fueron en\n",
            "la inteligencia.\n",
            "El rioble me hizo valle tan resistencia; y ahora te aún murió los años tan llenas de volor, gravedultes tan abandonar en no se nueva, la condive del objeto. Del que en su asciende a mi Guía, con una vió en este sitio donde yo\n",
            "le suplique. Pero las fijés inyestrección suya donde se detuvo\n",
            "al oír. Mas leemos descendido para nieve los hombres se regocijados los dos y tu mujeres bienaventurad\n"
          ]
        }
      ]
    }
  ]
}